---
title: "random forests"
author: "Hasti"
output: pdf_document
---

```{r pkgs, message=FALSE}
library(ROSE)
library(spdep)
library(tidyverse)
library(caTools)
library(ranger)
library(caret)
library(randomForest)
library(rpart.plot)
library(rpart)
library(pROC)
library(tinytex)
```

```{r cleanup}
file.remove("rf.RData")
```

```{r set_seed}
set.seed(101)  ## set random number sequence
```
inter data
```{r get_data}
## increase memory
if (.Platform$OS.type=="windows") memory.limit(1000000)

## get all data (as tables of x,y, predictors, change)
load("rr_points14.RData")

## test with smaller data set
dat <- filter(rr_points14[["2014"]],
              x<604000 & y >284000 &  y < 2846000)
```
Is there something funny about the `prop_build_nbrs` variable?
```{r prop_build}
pb <- dat$prop_build_nbrs
table(pb>0)
mean(pb>0,na.rm=TRUE) ## 10%
hist(pb[pb>0],breaks=50)
## there are no agri_nbrs
pa <- dat$prop_agri_nbrs
table(pa>0)
mean(pa>0,na.rm=TRUE)
## hist(pa[pa>0],breaks=50)
```
For some analyses we have to reduce the response to binary
previously: split into two separate data sets
here we lump "gain" and "loss" together into "change" ...
advantage: we only have to run one analysis (not two separate analyses, one
for gain/no gain and one for loss/no loss
disadvantage: it might not make geological sense to lump (variables leading to dune loss)
together with (variables leading to dune gain)
```{r upsample_data}
dat$change <- factor(dat$change, levels=0:3,
                     labels= c("no gain","gain","loss","no loss"))

## choice 1.
dat$change2 <- factor(ifelse(dat$change %in% c("no gain", "no loss"),
                                 "no change", "change"))

## choice 2.
dat_gain <- filter(dat, change %in% c("no gain", "gain")) 
dat_loss <- filter(dat, change %in% c("no loss", "loss"))

## analyzing gain only, first ...
if (file.exists("dat_upsample")) {load("dat_upsample")
} else {
dat_upsample <- ovun.sample(change ~ ., data = dat_gain, method = "both", N=1500)$data
}
```
test and train samples
```{r train_and_test_sample}
set.seed(123)
## pkgname::function()
## explicitly note which package a function comes from ...
set.seed(23489)
train_index <- sample(1:nrow(dat_upsample), 0.9 * nrow(dat_upsample))
sample_train <- dat_upsample[train_index, ]
sample_test  <- dat_upsample[-train_index, ]
head(sample_train)
dim(sample_train)
## a shorter way to do the same thing ...
## train <- subset(dat_upsample, sample)
## test  <- subset(dat_upsample,  !sample) ## ! means 'not'
```
Anytime we want to fit a model using train we tell it which model to fit by providing a formula for the first argument (as.factor(old) ~ . means that we want to model old as a function of all of the other variables). Then we need to provide a method (we specify "ranger" to implement randomForest).
Ranger is a fast implementation of random forests 
By default, the train function without any arguments re-runs the model over 25 bootstrap samples and across 3 options of the tuning parameter (the tuning parameter for ranger is mtry; the number of randomly selected predictors at each cut in the tree).
```{r fitted_random_forest, cache=TRUE}
rf_fit <- train(as.factor(change2) ~ .,
                data = sample_train,
                method = "ranger")
```
Prediction
```{r confusion_matrix}
rf_pred <- predict(rf_fit, sample_test)
con <- confusionMatrix(rf_pred, as.factor(sample_test$change2))
```
Preprocessing: estimates the required parameters for each operation 
```{r preproc}
## 1. with prop_agri_nbrs, without 'nzv': error
## 2. with prop_agri_nbrs, with 'nzv':
##  wants us to remove "prop_build_nbrs","prop_settle_nbrs","prop_agri_nbrs"
## 3. without prop_agri_nbrs, without 'nzv': fine ??
## 4. without prop_agri_nbrs, with 'nzv': wants us to remove the other two
##
## selected a subset that has relatively few built neighbors
##  preProcess is trying to remove variables with fewer than 5% of the minority
pb <- sample_train$prop_build_nbrs
table(pb>0)
length(unique(pb))
mean(pb>0,na.rm=TRUE) ## 3%
hist(pb[pb>0],breaks=50)

preprocess=preProcess(
    ## remove this ourselves!
    dplyr::select(sample_train, -prop_agri_nbrs),
    method = c("center", "scale",
               ## "nzv",
               "pca"),
    ## ??? how 
    ## freqCut=100/1, ## only cut variables if rare <1%
    ## uniqueCut=5
)
preprocess$method$remove

```
Removed= omit 3 predictors
Scaled=in ranger method the data to the interval between zero and one
```{r traindata}
sample_train2=subset(sample_train, select=- c(prop_build_nbrs, prop_settle_nbrs, prop_agri_nbrs))
dim(sample_train2)
```
random forest of data that preprocessed
```{r rf2,cache=TRUE}
rf_fit2 <- train(as.factor(change2) ~ ., data = sample_train2,
                method = "ranger")
sample_test2=subset(sample_test, select=- c(prop_build_nbrs, prop_settle_nbrs, prop_agri_nbrs))
rf_pred2 <- predict(rf_fit2, sample_test2)
con2=confusionMatrix(rf_pred2, as.factor(sample_test2$change2))
```
k-fold=generate training and testing sets
```{r make_a_group}
grouped=cbind(sample_train2[1:1350, ], group = rep(1:10, each = 5))
```
grouped folds(10 folds)
```{r make_fold}
group_folds=groupKFold(grouped$group, k = 10)
group_fit_control=trainControl(index = group_folds,method = "cv")
```
Random forest of new set of data:
```{r train_rf}
rf_fit3 <- train(as.factor(change2) ~ .,
                 data = sample_train2,
                 method = "ranger",
                 trControl = group_fit_control)
max(rf_fit3$results$Accuracy)
rf_pred3 <- predict(rf_fit3, sample_test)
con3=confusionMatrix(rf_pred3, as.factor(sample_test$change2))
```
Check overfitting: if training error is lower than our approximation of generalization error via the test error, we have over fitting
```{r accuracy_of_train_data}
rf_pred_train <- predict(rf_fit3, sample_train2)
contftrain <- confusionMatrix(rf_pred_train, as.factor(sample_train2$change2))
rf_pred_test <- predict(rf_fit3, sample_test2)
conftest=confusionMatrix(rf_pred3, as.factor(sample_test2$change2))
```
not fitted random forest
```{r not_fitted_random_forest,results="hide"}
## if you want to clear this file so that
##  we actually re-run the RF fit, then file.remove("rf.RData")
##  (or do it from your computer)
## we could also use cache=TRUE here
if (file.exists("rf.RData")) {
    load("rf.RData")
} else {
    rf <- randomForest(formula= factor(change2) ~ . - x - y - landuse - change,
                       data = sample_train, n.trees=250,interaction.depth=7,
                       ## do.trace=1,
                       type="classification", proximity=TRUE)
    save("rf",file="rf.RData")
}
```
importance of predictors
```{r importance}
importance(rf)
## type = 2 (impurity) because we're doing classification and not regression
varImpPlot(rf)
```
OOB error
```{r out_of_bag_error}
## rf$err.rate[,1]
plot(rf$err.rate[,1], type = "l")
```
tree Model
```{r tree_model}
set_tree = rpart(change2 ~ . -change -landuse -x-y,
                 data =sample_train)
rpart.plot(set_tree)
```

```{r side_by_side}
op <- par(mfrow=c(1,2))
varImpPlot(rf)
rpart.plot(set_tree)
par(op) ## restore original parameter
```

```{r ROC_AUC}
probs=predict(rf,
              newdata=sample_test,
              type="prob")
head(probs)
##Receiver Operating Characteristic(ROC)
## based on the true positive rate and false positive rate
rocCurve<- pROC::roc(sample_test$change2,probs[,"change"])
plot(rocCurve ,col=c(4))
###calculate the area under curve (bigger is better)
auc(rocCurve)
```
An example of non-independent test and train:
```{r nonind_tt}
plot(sample_train$x,sample_train$y)
points(sample_test$x,sample_test$y,col=2,pch=16)
legend("bottomleft",pch=c(1,16),col=c(1,2),
       legend=c("train","test"))
```

```{r nonind_tt}
plot(sample_train$x,sample_train$y)
points(sample_test$x,sample_test$y,col=2,pch=16)
legend("bottomleft",pch=c(1,16),col=c(1,2),
       legend=c("train","test"))
```

```{r}
bn <- rep(1:10, length.out=nrow(dat_upsample))
bns <- sample(bn)  ## permutation: random order
cvec <- palette.colors(10, "Tableau 10")
plot(dat_upsample$x, dat_upsample$y, col=cvec[bns], pch=16, type="n")
text(dat_upsample$x, dat_upsample$y, bns, col=cvec[bns],cex=2)
```
