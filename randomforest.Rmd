---
title: "random forests"
author: "Hasti"
---

You can type regular text, explanations, etc. outside of "code chunks"

```{r pkgs}
library(ROSE)
library(spdep)
library(tidyverse)
library(caTools)
library(ranger)
library(caret)
```

```{r get_data}
## get all data (as tables of x,y, predictors, change)
load("rr_points14.RData")

## test with smaller data set
dat <- filter(rr_points14[["2014"]],
                  x<604000 & y >284000 &  y < 2846000)
```

For some analyses we have to reduce the response to binary

previously: split into two separate data sets
here we lump "gain" and "loss" together into "change" ...
advantage: we only have to run one analysis (not two separate analyses, one
for gain/no gain and one for loss/no loss
disadvantage: it might not make geological sense to lump (variables leading to dune loss)
together with (variables leading to dune gain)

```{r}
dat$change <- factor(dat$change, levels=0:3,
                     labels= c("no gain","gain","loss","no loss"))

## choice 1.
dat$change2 <- factor(ifelse(dat$change %in% c("no gain", "no loss"),
                                 "no change", "change"))

## choice 2.
dat_gain <- filter(dat, change %in% c("no gain", "gain")) 
dat_loss <- filter(dat, change %in% c("no loss", "loss"))

## analyzing gain only, first ...
if (file.exists("dat_upsample")) {load("dat_upsample")
} else {
dat_upsample <- ovun.sample(change ~ ., data = dat_gain, method = "both", N=1500)$data
}
```
test and train samples
```{r}
set.seed(123)
## pkgname::function()
## explicitly note which package a function comes from ...
set.seed(23489)
train_index <- sample(1:nrow(dat_upsample), 0.9 * nrow(dat_upsample))
sample_train <- dat_upsample[train_index, ]
sample_test <- dat_upsample[-train_index, ]
head(sample_train)
dim(sample_train)
## a shorter way to do the same thing ...
## train <- subset(dat_upsample, sample)
## test  <- subset(dat_upsample,  !sample) ## ! means 'not'
```

Anytime we want to fit a model using train we tell it which model to fit by providing a formula for the first argument (as.factor(old) ~ . means that we want to model old as a function of all of the other variables). Then we need to provide a method (we specify "ranger" to implement randomForest).
Ranger is a fast implementation of random forests 
By default, the train function without any arguments re-runs the model over 25 bootstrap samples and across 3 options of the tuning parameter (the tuning parameter for ranger is mtry; the number of randomly selected predictors at each cut in the tree).

```{r}
## use train data
rf_fit <- train(as.factor(change2) ~ ., data = sample_train,
                method = "ranger")
```
predict the outcome on a test set

```{r confusion matrix}
rf_pred <- predict(rf_fit, sample_test)
con=confusionMatrix(rf_pred, as.factor(sample_test$change2))
```