---
title: "Cross validation"
---

```
## 0: not-erg before and after (no gain)
## 1: not-erg before, erg after (gain)
## 2: erg before, not-erg after (loss)
## 3: erg before and after (no loss)
```

```{r packages}
library(tidyverse) ## load filter(), ... function
library(blockCV)
library(sf)
library(raster)
library(ggplot2)
library(shiny) ## what for ... ??
library(ROSE)
library(caret)
source("functions.R")
library(randomForest)
library(rpart.plot)
library(spdep)
```
To knit "by hand", `rmarkdown::render("Cross_validation.Rmd")`
```{r input_data}
## increase memory
if (.Platform$OS.type=="windows") memory.limit(1000000)

## get all data (as tables of x,y, predictors, change)
load("rr_points14.RData")

## test with smaller data set
dat <- dplyr::filter(rr_points14[["2014"]],
                     x<604000 & y >284000 &  y < 2846000)
dat$change2 <- change_fun(dat$change)
```

```{r upsample}
dat_gain <- dplyr::filter(dat, change2 %in% c("no gain", "gain")) 

## analyzing gain only, first ...
if (file.exists("dat_upsample")) {
    load("dat_upsample")
}  else {
    dat_upsample <- ROSE::ovun.sample(change ~ ., data = dat_gain, method = "both", N=1500)$data
}
```


```{r classified_maps}
Classified1=raster("Classified1.tif")
Classified2=raster("classified2.tif")
```
The appropriate format of species data for the blockCV package is simple features (sf) or SpatialPointsDataFrame.pamake a SpatialPointsDataFrame object from data.frame
```{r numeric_data}
PA_data <- st_as_sf(dat_upsample, coords = c("x", "y"), crs = crs(Classified1))
```

```{r train_vs_test}
set.seed(23489)
train_index <- sample(1:nrow(PA_data), 0.9 * nrow(dat_upsample))
sample_train <- PA_data[train_index, ]
sample_test  <- PA_data[-train_index, ]
```
Spatial blocking by specified range with random assignment
creates spatially separated folds based on a pre-specified distance (cell size of the blocks).
```{r spatial_blocking, cache=TRUE}
sb <- spatialBlock(speciesData = PA_data,
                   species = "change2",
                   rasterLayer = Classified1,
                   ## theRange = 70000, # size of the blocks
                   theRange = 5000, # size of the blocks                   
                   k = 5,
                   selection = "random",
                   iteration = 100, # find evenly dispersed folds
                   biomod2Format = TRUE,
                   xOffset = 0, # shift the blocks horizontally
                   yOffset = 0)
## select *first* (training) set of indices for each fold
## IS THIS THE RIGHT WAY TO DO THIS ???
trainfoldsb <- lapply(sb$folds, function(x) x[[1]])
traincontrolsb <- caret::trainControl(index = trainfoldsb,method="CV")
```
specify sets of similar environmental conditions based on the input covariates. Species data corresponding to any of these groups or clusters are assigned to a fold.
```{r envBlock}
eb <- envBlock(rasterLayer = Classified1,
               speciesData = PA_data,
               species = "change2",
               k = 5,
               standardization = "standard", # rescale variables between 0 and 1
               rasterBlock = FALSE,
               numLimit = 50)
trainfoldeb <- lapply(eb$folds, function(x) x[[1]])
traincontroleb <- caret::trainControl(index = trainfoldeb,method="CV")
```
hP=index` should be lists of integers.
adding points on saptialBlock plot
```{r random_forest_ebfolds}
rf_eb=train(as.factor(change2) ~ .,
              data = dat_upsample, ## use the same data here as we used in spatialBlock()?
              method = "ranger",
              trControl = traincontroleb)
```
The function works by automatically fitting variograms to each continuous raster and finding the effective range of spatial autocorrelation. 
```{r range_of_spatial_autocorrelation}
range3 <- spatialAutoRange(rasterLayer = Classified1,
                           sampleNumber = 5000,
                           doParallel = FALSE,
                           progress = TRUE)
```

generates spatially separated training and testing folds by considering buffers of specified distance around each observation point. 
```{r define_buffers}
set.seed(101)
bf <- buffering(speciesData = PA_data,
                  theRange = 1000,#ideally the range of spatial autocorrelation
                  species = "change2", # to count the number of presences and absences/backgrounds
                  spDataType = "PA", # presence-absence  data type
                  progress = TRUE)
```

```{r plot_buffer_1}
dd1 <- bf$folds[[1]] ## first fold

## double-check correspondence of data ...
head(PA_data$geometry)
head(dat_upsample[,c("x","y")])
## seems OK?

## training data
plot(y~x, data=dat_upsample[dd1[[1]],],
     xlim=range(dat_upsample$x),
     ylim=range(dat_upsample$y))
## testing data
## data points that aren't in test or in train
excluded <- setdiff(seq(nrow(dat_upsample)), c(dd1[[1]],dd1[[2]]))
##plot
with(dat_upsample[dd1[[2]],], points(x,y,col=2,pch=16,cex=2))
with(dat_upsample[excluded,], points(x,y,col=4))
```

```{r random_forest_buffer1}
# extract the folds (list)SpatialBlock object 
folds <- bf$folds
fvec <- seq_len(length(folds))
for (k in fvec) {
    ## extracting the training and testing indices
    ## this way works with folds list (but not foldID)
    trainSet <- unlist(folds[[k]][1]) # training set indices
    testSet <- unlist(folds[[k]][2]) # testing set indices
}
## BMB: when this finishes, we have
trainSet <- unlist(folds[[1500]][1])
testSet <- unlist(folds[[1500]][2])

## if you want to run the rf for lots of folds
## rf_list <- list()
## for (k in 1:nfolds) {
##    ## extract train set
##    ## run rf on train set
##    rf_list[[k]] <- ## random forest model you just fitted
## }
## if you have 10 folds, this will save 10 fitted random forest models

## aucvec <- rep(NA,nfolds)
## for (k in 1:nfolds) {
##   get test set data for fold k
##   get predictions from rf_list[[k]]
##   pp <- pROC::roc
##   aucvec[k] <- c(pp$auc)
## }
## mean(aucvec)
##
## averaging the ROC curves is harder ...
a <- PA_data[trainSet,]
a$geometry <- NULL
b <- PA_data[testSet, ]
b$geometry <- NULL
rfb1 <- randomForest(formula= factor(change2) ~ .  - landuse - change,
                       data = a, n.trees=250,interaction.depth=7,
                       ## do.trace=1,
                     type="classification", proximity=TRUE) # model fitting on training set
## test set is only 1 point
predb1<- predict(rfb1, newdata=b, type = "prob")
head(predb1)
```

If we want to do things like plot ROC curves for these data, we have to run the random forest lots of times and save the test set and predictions for every fold ...

```{r many_folds}
nfolds <- 5 ## not going to do all 1500 folds because it will be way too slow!
rf_predprob <- rep(NA, nfolds)
rf_obs <- rep(NA, nfolds)
for (k in 1:nfolds) {
    cat(k,"\n")
    trainSet <- unlist(folds[[k]][1]) # training set indices
    testSet <- unlist(folds[[k]][2]) # testing set indices
    a <- PA_data[trainSet,]
    a$geometry <- NULL
    b <- PA_data[testSet, ]
    b$geometry <- NULL
    rfb_k <- randomForest(formula= factor(change2) ~ .  - landuse - change,
                         data = a, n.trees=250,interaction.depth=7,
                         type="classification", proximity=TRUE)
    ## first row, "gain" column probability
    pp <- predict(rfb_k, newdata=b, type = "prob")
    print(pp)
    rf_predprob[k] <-  pp[1,"gain"]
    rf_obs[k] <- b$change2
}
```


```{r plot}
sb$plots + geom_sf(data = PA_data, alpha = 0.5)
```
visualising the generated folds on a map
```{r folds_in_spatialk}
if (FALSE) {
    fold=foldExplorer(eb, Classified1, PA_data)
    print(fold)
}
```
buffer based on the defined boders=buffer 2
```{r buffer_2}
##not convenience
ctr <- list(x=601023, y=2844924)  ## a focal point
dist_to_ctr <- sqrt((dat_upsample$x-ctr$x)^2 + (dat_upsample$y-ctr$y)^2)
pts <- seq(nrow(dat_upsample))
test <- pts[dist_to_ctr<1000]
excluded <- pts[dist_to_ctr>1000 & dist_to_ctr<2000]
train <- pts[dist_to_ctr>2000]
plot(y~x,data=dat_upsample[train,])
grid(nx=4,ny=4,col=5)
with(dat_upsample[excluded,], points(x,y,col=4))
with(dat_upsample[test,], points(x,y,col=2,pch=16))
```
buffer based on the defined boders=buffer 3
```{r determine_data}
##introsucing the specific block 
corners <- list(x=c(601500,605000),
                y=c(2840000,2837000))
pts <- seq(nrow(dat_upsample))
##introducinf test data in the introduced block
test <- pts[dat_upsample$x > corners$x[1] &
            dat_upsample$x < corners$x[2] &
            dat_upsample$y < corners$y[1] &
            dat_upsample$y > corners$y[2]]

##setdiff indicates which elements of a vector or data frame X are not existent in a vector or data frame Y.
train <- setdiff(pts,test)
plot(y~x,data=dat_upsample[train,])
grid(nx=4,ny=4,col=5)
with(dat_upsample[test,], points(x,y,col=2,pch=16))
```
determine train and test data 
```{r train_testdata}
##number of k
umFolds <- trainControl(method = "cv", number = 10)
##trainand
train2 <- PA_data[train,]
test2 <- PA_data[test, ]
train2$geometry <- NULL
test2$geometry <- NULL
```

Random forest

```{r random_forest_buffer3}
rfb3 <- randomForest(formula= factor(change2) ~ . - landuse - change,
                       data = train2, n.trees=250,interaction.depth=7,
                       ## do.trace=1,
                       type="classification", proximity=TRUE, trcontrol=umFolds)
plot(rfb3)
varImpPlot(rfb3)
predb3 <- predict(rfb3, newdata=test2, typeprobs="prob")
conb <- confusionMatrix(predb3, as.factor(test2$change2))
table3=table(test2$change2, predb3)
set_tree = rpart(change2 ~ . -change -landuse ,
                 data =train2)
rpart.plot(set_tree)
```

```{r roc_random_forest_buffer3_roc, message=TRUE}
ff <-  factor(test2$change,
              ## 0 = no gain,  1 = gain
              levels=c(0,1),  ## these are the values in the original vector
              labels=c("no gain","gain") ## these will be the names in the new vector
              )
table(test2$change)
table(ff)
## convert a two-level factor back to 0/1
to_binary <- function(x) { as.numeric(x) -1 }
rocCurve <- pROC::roc(to_binary(ff),
                      ## predb3[,"change"]
                      to_binary(predb3))
## gain/no gain  vs.  change/no change
plot(rocCurve)

## undebug(pROC:::roc.default)


pp <- pROC::roc(sample(0:1,size=50,replace=TRUE), pred=runif(50))
plot(pp)
plot(pROC::roc(c(1,1,0,0), pred=c(0.2,0.4,0.1,0.2)))
plot(pROC::roc(c(1,0), pred=c(0.2,0.2)))
## we know this won't work because it only has one data point,
## we need at least one observation in each level
try(plot(pROC::roc(factor(1, levels=0:1), pred=0.2)), silent=TRUE)
 

plot(rocCurve ,col=c(4))
###calculate the area under curve (bigger is better)
## extract AUC and drop extra information
auc <- function(x) c(x$auc)
auc(rocCurve)
```

```{r roc_random_forest_buffer1}
fg <- factor(b$change2,levels=c("gain","nogain"))
str(fg)
str(predb1[,"gain"])
## try() = don't stop if you encounter an error 
try(rocCurve <- pROC::roc(fg, predb1[,"gain"]))
## what are the variables 'b', and 'predb1' ?

pp <- pROC::roc(sample(0:1,size=50,replace=TRUE), pred=runif(50))
plot(pp)
plot(pROC::roc(c(1,1,0,0), pred=c(0.2,0.4,0.1,0.2)))
plot(pROC::roc(c(1,0), pred=c(0.2,0.2)))


plot(rocCurve ,col=c(4))
###calculate the area under curve (bigger is better)
auc(rocCurve)
```
may be below method is not correct for cross validation

```{r random_forest_sbfolds}
## leave out x and y, manually (ugh) (formula   ~ . - x - y doesn't seem to work?)
dat_upsample_noxy <- dat_upsample[!names(dat_upsample) %in% c("x","y")]
rf_sb=train(as.factor(change2) ~ .,
            ## don't include lat/long in the model
            data = dat_upsample_noxy, ## use the same data here as we used in spatialBlock()?
            method = "ranger",
            trControl = traincontrolsb)

rf_predsb <- predict(rf_sb, sample_test)

consb=confusionMatrix(rf_predsb, as.factor(sample_test$change2))
```

Residual & Moran I

```{r resid_moranI}
testpoint <- SpatialPointsDataFrame(cbind(dat_gain$x, dat_gain$y), dat_gain)
lstw  <- spdep::nb2listw(knn2nb(knearneigh(testpoint, k = 10)))
## convert factor to 0/1 variable
mk_num_obs <- function(x) ifelse(x=="no gain", 0, 1)
num_obs <- mk_num_obs(dat_gain$change2)
num_pred <- mk_num_obs(predict(rfb3, newdata=dat_gain))
## sum(is.na(num_pred)) ## ?? why ??
good_pred <- which(!is.na(num_pred))
num_obs <- num_obs[good_pred]
num_pred <- num_pred[good_pred]
res2 = num_obs - num_pred
lstw2 <- lstw ## make a copy
lstw2$neighbours <- lstw2$neighbours[good_pred]
lstw2$weights <- lstw2$weights[good_pred]
attributes(lstw2$weights) <- attributes(lstw$weights)
length(res2)
length(lstw2$neighbours)
moran.test(res2, lstw2)
```

