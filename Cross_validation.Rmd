---
title: "Cross validation"
---

```{r packages}
library(tidyverse) ## load filter(), ... function
library(blockCV)
library(sf)
library(raster)
library(ggplot2)
library(shiny) ## what for ... ??
library(ROSE)
library(caret)
source("functions.R")
library(randomForest)
library(rpart.plot)
```
To knit "by hand", `rmarkdown::render("Cross_validation.Rmd")`
```{r input_data}
## increase memory
if (.Platform$OS.type=="windows") memory.limit(1000000)

## get all data (as tables of x,y, predictors, change)
load("rr_points14.RData")

## test with smaller data set
dat <- dplyr::filter(rr_points14[["2014"]],
                     x<604000 & y >284000 &  y < 2846000)
dat$change2 <- change_fun(dat$change)
```

```{r upsample}
dat_gain <- dplyr::filter(dat, change2 %in% c("no gain", "gain")) 

## analyzing gain only, first ...
if (file.exists("dat_upsample")) {
    load("dat_upsample")
}  else {
    dat_upsample <- ROSE::ovun.sample(change ~ ., data = dat_gain, method = "both", N=1500)$data
}
```


```{r classified_maps}
Classified1=raster("Classified1.tif")
Classified2=raster("classified2.tif")
```
The appropriate format of species data for the blockCV package is simple features (sf) or SpatialPointsDataFrame.pamake a SpatialPointsDataFrame object from data.frame
```{r numeric_data}
PA_data <- st_as_sf(dat_upsample, coords = c("x", "y"), crs = crs(Classified1))
```

```{r train_vs_test}
set.seed(23489)
train_index <- sample(1:nrow(PA_data), 0.9 * nrow(dat_upsample))
sample_train <- PA_data[train_index, ]
sample_test  <- PA_data[-train_index, ]
```
Spatial blocking by specified range with random assignment
creates spatially separated folds based on a pre-specified distance (cell size of the blocks).
```{r spatial_blocking, cache=TRUE}
sb <- spatialBlock(speciesData = PA_data,
                   species = "change2",
                   rasterLayer = Classified1,
                   ## theRange = 70000, # size of the blocks
                   theRange = 5000, # size of the blocks                   
                   k = 5,
                   selection = "random",
                   iteration = 100, # find evenly dispersed folds
                   biomod2Format = TRUE,
                   xOffset = 0, # shift the blocks horizontally
                   yOffset = 0)
## select *first* (training) set of indices for each fold
## IS THIS THE RIGHT WAY TO DO THIS ???
trainfoldsb <- lapply(sb$folds, function(x) x[[1]])
traincontrolsb <- caret::trainControl(index = trainfoldsb,method="CV")
```
specify sets of similar environmental conditions based on the input covariates. Species data corresponding to any of these groups or clusters are assigned to a fold.
```{r envBlock}
eb <- envBlock(rasterLayer = Classified1,
               speciesData = PA_data,
               species = "change2",
               k = 5,
               standardization = "standard", # rescale variables between 0 and 1
               rasterBlock = FALSE,
               numLimit = 50)
trainfoldeb <- lapply(eb$folds, function(x) x[[1]])
traincontroleb <- caret::trainControl(index = trainfoldeb,method="CV")
```
hP=index` should be lists of integers.
adding points on saptialBlock plot
```{r random_forest_ebfolds}
rf_eb=train(as.factor(change2) ~ .,
              data = dat_upsample, ## use the same data here as we used in spatialBlock()?
              method = "ranger",
              trControl = traincontroleb)
```
The function works by automatically fitting variograms to each continuous raster and finding the effective range of spatial autocorrelation. 
```{r range_of_spatial_autocorrelation}
range3 <- spatialAutoRange(rasterLayer = Classified1,
                           sampleNumber = 5000,
                           doParallel = FALSE,
                            progress = TRUE)
```
generates spatially separated training and testing folds by considering buffers of specified distance around each observation point. 
```{r define_buffers}
set.seed(101)
bf <- buffering(speciesData = PA_data,
                  theRange = 1000,#ideally the range of spatial autocorrelation
                  species = "change2", # to count the number of presences and absences/backgrounds
                  spDataType = "PA", # presence-absence  data type
                  progress = TRUE)
```

```{r plot_buffer_1}
dd1 <- bf$folds[[1]] ## first fold

## double-check correspondence of data ...
head(PA_data$geometry)
head(dat_upsample[,c("x","y")])
## seems OK?

## training data
plot(y~x, data=dat_upsample[dd1[[1]],],
     xlim=range(dat_upsample$x),
     ylim=range(dat_upsample$y))
## testing data
## data points that aren't in test or in train
excluded <- setdiff(seq(nrow(dat_upsample)), c(dd1[[1]],dd1[[2]]))
##plot
with(dat_upsample[dd1[[2]],], points(x,y,col=2,pch=16,cex=2))
with(dat_upsample[excluded,], points(x,y,col=4))
```

```{r random_forest_buffer1}
# extract the folds (list)SpatialBlock object 
folds <- bf$folds
fvec <- seq_len(length(folds))
for (k in fvec) {
    ## extracting the training and testing indices
    ## this way works with folds list (but not foldID)
    trainSet <- unlist(folds[[k]][1]) # training set indices
    testSet <- unlist(folds[[k]][2]) # testing set indices
}
## BMB: when this finishes, we have
trainSet <- unlist(folds[[1500]][1])
testSet <- unlist(folds[[1500]][2])

## if you want to run the rf for lots of folds
## rf_list <- list()
## for (k in 1:nfolds) {
##    ## extract train set
##    ## run rf on train set
##    rf_list[[k]] <- ## random forest model you just fitted
## }
## if you have 10 folds, this will save 10 fitted random forest models

## aucvec <- rep(NA,nfolds)
## for (k in 1:nfolds) {
##   get test set data for fold k
##   get predictions from rf_list[[k]]
##   pp <- pROC::roc
##   aucvec[k] <- c(pp$auc)
## }
## mean(aucvec)
##
## averaging the ROC curves is harder ...


a <- PA_data[trainSet,]
a$geometry <- NULL
b <- PA_data[testSet, ]
b$geometry <- NULL
rfb1 <- randomForest(formula= factor(change2) ~ .  - landuse - change,
                       data = a, n.trees=250,interaction.depth=7,
                       ## do.trace=1,
                     type="classification", proximity=TRUE) # model fitting on training set
## test set is only 1 point
predb1<- predict(rfb1, newdata=b, type = "prob")
head(predb1)
```

```{r plot}
sb$plots + geom_sf(data = PA_data, alpha = 0.5)
```
visualising the generated folds on a map
```{r folds_in_spatialk}
if (FALSE) {
    fold=foldExplorer(eb, Classified1, PA_data)
    print(fold)
}
```
buffer based on the defined boders=buffer 2
```{r buffer_2}
##not convenience
ctr <- list(x=601023, y=2844924)  ## a focal point
dist_to_ctr <- sqrt((dat_upsample$x-ctr$x)^2 + (dat_upsample$y-ctr$y)^2)
pts <- seq(nrow(dat_upsample))
test <- pts[dist_to_ctr<1000]
excluded <- pts[dist_to_ctr>1000 & dist_to_ctr<2000]
train <- pts[dist_to_ctr>2000]
plot(y~x,data=dat_upsample[train,])
grid(nx=4,ny=4,col=5)
with(dat_upsample[excluded,], points(x,y,col=4))
with(dat_upsample[test,], points(x,y,col=2,pch=16))
```
buffer based on the defined boders=buffer 3
```{r determine_data}
##introsucing the specific block 
corners <- list(x=c(601500,605000),
                y=c(2840000,2837000))
pts <- seq(nrow(dat_upsample))
##introducinf test data in the introduced block
test <- pts[dat_upsample$x > corners$x[1] &
            dat_upsample$x < corners$x[2] &
            dat_upsample$y < corners$y[1] &
            dat_upsample$y > corners$y[2]]

##setdiff indicates which elements of a vector or data frame X are not existent in a vector or data frame Y.
train <- setdiff(pts,test)
plot(y~x,data=dat_upsample[train,])
grid(nx=4,ny=4,col=5)
with(dat_upsample[test,], points(x,y,col=2,pch=16))
```
determine train and test data 
```{r train_testdata}
##number of k
umFolds <- trainControl(method = "cv", number = 10)
##trainand
train2 <- PA_data[train,]
test2 <- PA_data[test, ]
train2$geometry <- NULL
test2$geometry <- NULL
```
p
```{r}
rfb3=randomForest(formula= factor(change2) ~ . - landuse - change,
                       data = train2, n.trees=250,interaction.depth=7,
                       ## do.trace=1,
                       type="classification", proximity=TRUE, trcontrol=umFolds)
plot(rfb3)
varImpPlot(rfb3)
predb3 <- predict(rfb3, newdata=test2, typeprobs="prob")
conb <- confusionMatrix(predb3, as.factor(test2$change2))
table3=table(test2$change2, predb3)
set_tree = rpart(change2 ~ . -change -landuse ,
                 data =train2)
rpart.plot(set_tree)
```
H-P:Error in ROC
```{r roc_random_forest_buffer3, message=TRUE}
ff <-  factor(test2$change,
              ## 0 = no gain,  1 = gain
              levels=c(0,1),  ## these are the values in the original vector
              labels=c("no gain","gain") ## these will be the names in the new vector
              )
table(test2$change)
table(ff)
## convert a two-level factor back to 0/1
to_binary <- function(x) { as.numeric(x) -1 }
rocCurve <- pROC::roc(to_binary(ff),
                      ## predb3[,"change"]
                      to_binary(predb3))
## gain/no gain  vs.  change/no change
plot(rocCurve)

## undebug(pROC:::roc.default)


pp <- pROC::roc(sample(0:1,size=50,replace=TRUE), pred=runif(50))
plot(pp)
plot(pROC::roc(c(1,1,0,0), pred=c(0.2,0.4,0.1,0.2)))
plot(pROC::roc(c(1,0), pred=c(0.2,0.2)))


plot(rocCurve ,col=c(4))
###calculate the area under curve (bigger is better)
## extract AUC and drop extra information
auc <- function(x) c(x$auc)
auc(rocCurve)
```
H-P:Error in predcv[, "gain"] : subscript out of bounds
```{r roc_random_forest_buffer1}
rocCurve <- pROC::roc(factor(b$change2,levels=c("gain","nogain")),
                      predb1[,"gain"])

pp <- pROC::roc(sample(0:1,size=50,replace=TRUE), pred=runif(50))
plot(pp)
plot(pROC::roc(c(1,1,0,0), pred=c(0.2,0.4,0.1,0.2)))
plot(pROC::roc(c(1,0), pred=c(0.2,0.2)))


plot(rocCurve ,col=c(4))
###calculate the area under curve (bigger is better)
auc(rocCurve)
```
may be below method is not correct for cross validation
```{r random_forest_sbfolds}
rf_sb=train(as.factor(change2) ~ .,
              data = dat_upsample, ## use the same data here as we used in spatialBlock()?
              method = "ranger",
              trControl = traincontrolsb)

rf_predsb <- predict(rf_sb, sample_test)
consb=confusionMatrix(rf_predsb, as.factor(sample_test$change2))
```
residual & Moran I
```{r}
testpoint <- SpatialPointsDataFrame(cbind(dat_gain$x, dat_gain$y), dat_gain)
lstw  <- nb2listw(knn2nb(knearneigh(testpoint, k = 10)))
res=train2$change2 - predict(rfb3) # residuals
train2$change2 - predict(rfb3) # residuals
res2=as.numeric(res)
moran.test(res2, lstw)
```

